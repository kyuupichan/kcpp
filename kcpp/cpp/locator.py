# Copyright (c) 2025, Neil Booth.
#
# All rights reserved.
#
'''The locator handles the details of token locations - whether they come from source files,
and if so which one, the include stack at the time of that location, the macro stack at the
time of expansion, etc.'''

from bisect import bisect_left
from dataclasses import dataclass
from enum import IntEnum, auto

from ..basic import Buffer, BufferCoords
from ..diagnostics import (
    BufferRange, TokenRange, SpellingRange, DiagnosticContext, DID, location_none,
    RangeCoords,
)


__all__ = ['Locator']


@dataclass(slots=True)
class LineRange:
    '''Names a range of lines in a source file.  A new BufferRange entry creates an initial
    instance.  Subsequent entries are created by #line directives.'''
    start: int           # The first location in this range
    name: str            # Normally a file name, but can be e.g. <scratch>
    line_number: int


class BufferSpan:
    '''Represents the span of locations used for a source file being processed.  If a source
    file is processed more than once (for example, if it is included twice), each usage
    gets its own span.

    An include stack is formed through parent_loc.  This points to the location of the include
    directive (the actual "include" token) so that an include stack can be produced.
    '''

    def __init__(self, buffer, start, parent_loc, name):
        self.buffer = buffer
        self.start = start
        # End is inclusive, so this permits an end-of-buffer location
        self.end = start + len(buffer.text)
        self._parent_loc = parent_loc
        self.line_ranges = [LineRange(start, name, 1)]

    def macro_parent_loc(self, loc):
        assert self.start <= loc <= self.end
        return -1

    def spelling_loc(self, loc):
        return loc

    def add_line_range(self, start, name, line_number):
        assert start < self.line_ranges[-1].start < start <= self.end
        self.line_ranges.append(LineRange(start, name, line_number))

    def filename(self):
        return self.line_ranges[0].name


class ScratchBufferSpan(Buffer):
    '''A scratch buffer holds the spelling of virtual tokens that are generated by the macro
    expansion process - namely concatenated tokens and stringized tokens.

    The locator creates scratch buffers on demand; there may be several or none in
    existence.  A scratch buffer cannot change size once craeted, as its span of locations
    is fixed and not unbounded.  The scratch buffer keeps track of the origin of the
    virtual tokens it creates so that the macro stack can be correctly reported.
    '''

    def __init__(self, start, end):
        '''Create a scratch buffer with the given size.'''
        super().__init__(bytearray())
        assert start <= end
        self.buffer = self
        self.start = start
        self.end = end
        # Naturally sorted by offset.
        self.entries = []

    def size(self):
        return self.end - self.start + 1

    def add_spelling(self, spelling, parent_loc, entry_kind):
        start = len(self.buffer.text)
        if start + len(spelling) + 1 < self.size():
            # Add the spelling and a newline character (so it appears on its own line in
            # diagnostics)
            self.buffer.text.extend(spelling)
            self.buffer.text.append(10)
            self.entries.append(ScratchEntry(start, parent_loc, entry_kind))
            # Clear the cached line offsets
            self.buffer._sparse_line_offsets = None
            return start
        return -1

    def did_and_substitutions(self, pp, loc):
        kind = self.entry_for_loc(loc).kind
        if kind == ScratchEntryKind.concatenate:
            return DID.in_token_concatenation, []
        else:
            return DID.in_argument_stringizing, []

    def entry_for_loc(self, loc):
        '''Return the parent location (i.e. the location of the ## or # token) of a scratch buffer
        location.  loc is an offset into the scratch buffer.
        '''
        loc -= self.start
        assert 0 <= loc < len(self.buffer.text)
        return self.entries[bisect_left(self.entries, loc + 1, key=lambda e: e.offset) - 1]

    def spelling_loc(self, loc):
        return loc

    def macro_parent_loc(self, loc):
        return self.entry_for_loc(loc).parent_loc

    def filename(self):
        return '<scratch>'


class ScratchEntryKind(IntEnum):
    concatenate = auto()
    stringize = auto()


@dataclass(slots=True)
class ScratchEntry:
    offset: int
    parent_loc: int
    kind: ScratchEntryKind


@dataclass(slots=True)
class ObjectLikeMacroReplacementSpan:

    macro: object
    invocation_loc: int
    start: int
    end: int

    def spelling_loc(self, loc):
        token_index = loc - self.start
        return self.macro.replacement_list[token_index].loc

    def macro_parent_loc(self, loc):
        return self.invocation_loc

    def macro_name(self, pp):
        '''Return the macro name (as UTF-8).'''
        return pp.token_spelling_at_loc(self.invocation_loc)

    def did_and_substitutions(self, pp, loc):
        return DID.in_expansion_of_macro, [self.macro_name(pp)]


@dataclass(slots=True)
class FunctionLikeMacroReplacementSpan:

    invocation_loc: int
    start: int
    end: int
    locations: list

    def spelling_loc(self, loc):
        return self.locations[loc - self.start]

    def macro_parent_loc(self, loc):
        return self.locations[loc - self.start]

    def macro_name(self, pp):
        '''Return the macro name (as UTF-8).'''
        return pp.token_spelling_at_loc(self.invocation_loc)

    def did_and_substitutions(self, pp, loc):
        return DID.in_expansion_of_macro, [self.macro_name(pp)]


@dataclass(slots=True)
class MacroContext:
    macro_loc: int
    span: object


class Locator:
    '''Manages and supplies token locations.'''

    FIRST_BUFFER_LOC = 1
    FIRST_MACRO_LOC = 1 << 40

    def __init__(self, pp):
        self.pp = pp
        self.buffer_spans = []
        self.macro_spans = []
        self.scratch_range = None

    def new_buffer_loc(self, buffer, name, parent_loc):
        buffer_spans = self.buffer_spans
        if buffer_spans:
            start = buffer_spans[-1].end + 1
        else:
            start = self.FIRST_BUFFER_LOC
        buffer_spans.append(BufferSpan(buffer, start, parent_loc, name))
        return start

    def next_macro_span_start(self):
        try:
            return self.macro_spans[-1].end + 1
        except IndexError:
            return self.FIRST_MACRO_LOC

    def functionlike_macro_replacement_span(self, parent_loc, locations):
        start = self.next_macro_span_start()
        end = start + len(locations) - 1
        self.macro_spans.append(FunctionLikeMacroReplacementSpan(parent_loc, start, end,
                                                                 locations))
        return start

    def objlike_macro_replacement_span(self, macro, parent_loc):
        start = self.next_macro_span_start()
        end = start + len(macro.replacement_list) - 1
        self.macro_spans.append(ObjectLikeMacroReplacementSpan(macro, parent_loc, start, end))
        return start

    def new_scratch_token(self, spelling, parent_loc, entry_kind):
        def alloc_in_current(spelling):
            if self.scratch_range:
                result = self.scratch_range.add_spelling(spelling, parent_loc, entry_kind)
                if result != -1:
                    return self.scratch_range.start + result
            return -1

        assert isinstance(entry_kind, ScratchEntryKind)
        loc = alloc_in_current(spelling)
        if loc == -1:
            size = max(len(spelling), 1_000)
            start = self.buffer_spans[-1].end
            self.scratch_range = ScratchBufferSpan(start, start + size - 1)
            self.buffer_spans.append(self.scratch_range)
            loc = alloc_in_current(spelling)
            assert loc != -1
        return loc

    def lookup_span(self, loc):
        if loc >= self.FIRST_MACRO_LOC:
            spans = self.macro_spans
        else:
            spans = self.buffer_spans
        n = bisect_left(spans, loc + 1, key=lambda lr: lr.start) - 1
        span = spans[n]
        assert span.start <= loc <= span.end, f'{span.start} <= {loc} <= {span.end} {span}'
        return span

    def spelling_span_and_offset(self, loc):
        '''Return a pair (span, offset) where span is a BufferSpan or ScratchBufferSpan
        instance.'''
        while True:
            span = self.lookup_span(loc)
            spelling_loc = span.spelling_loc(loc)
            if spelling_loc != loc:
                # Object-like macros need to recurse once.  Function-like macros may be
                # many times.
                loc = spelling_loc
                continue
            return span, loc - span.start

    def spelling_buffer_and_offset(self, loc):
        '''Return a buffer and offset into it so that the token can be lexed.'''
        span, offset = self.spelling_span_and_offset(loc)
        return span.buffer, offset

    def buffer_span_loc(self, loc):
        '''Step up through the parents of a location until a BufferSpan is reached, and return the
        location there.

        So if a location arises from a macro expansion, this will return to the outermost
        macro invocation.
        '''
        while True:
            span = self.lookup_span(loc)
            parent_loc = span.macro_parent_loc(loc)
            if parent_loc == -1:
                return loc
            loc = parent_loc

    def source_file_coords(self, loc):
        '''Step through the parents of a location until a BufferSpan is reached, and convert the
        resulting location to coordinates of that token's spelling.  The will be the
        original location if directly in a source file, otherwise the outermost macro
        invocation.

        Note how this is different to the buffer containing the spelling of the token with
        location loc - use spelling_span_and_offset() to get that.
        '''
        return self.spelling_coords(self.buffer_span_loc(loc))

    def spelling_coords(self, loc, token_end=False):
        '''Convert a location to a BufferCoords instance.'''
        span, offset = self.spelling_span_and_offset(loc)
        if token_end:
            offset += self.token_length(span.start + offset)
        buffer = span.buffer
        line_offset, line_number = buffer.offset_to_line_info(offset)
        filename = span.filename()
        return BufferCoords(buffer, filename, line_number, offset - line_offset, line_offset)

    def token_length(self, loc):
        '''The length of the token in bytes in the source file.  This incldues, e.g., escaped
        newlines.  The result can be 0, for end-of-source indicator EOF.
        '''
        lexer = self.pp.lexer_at_loc(loc)
        prior_cursor = lexer.cursor
        lexer.get_token_quietly()
        return lexer.cursor - prior_cursor

    def range_coords(self, source_range):
        if isinstance(source_range, SpellingRange):
            # Convert the SpellingRange to a BufferRange
            assert source_range.start < source_range.end
            lexer = self.pp.lexer_at_loc(source_range.token_loc)
            cursor = lexer.cursor
            lexer.get_token_quietly()
            offsets = [source_range.start, source_range.end]
            # FIXME: this is ugly, find something better
            lexer.utf8_spelling(cursor, lexer.cursor, offsets)
            source_range = BufferRange(offsets[0], offsets[1])

        if isinstance(source_range, BufferRange):
            start = self.spelling_coords(source_range.start)
            end = self.spelling_coords(source_range.end)
            assert start.buffer is end.buffer
        elif isinstance(source_range, TokenRange):
            if source_range.start <= location_none:
                start = end = None
            else:
                start = self.spelling_coords(source_range.start)
                end = self.spelling_coords(source_range.end, True)
        elif source_range is None:
            start = end = None
        else:
            raise RuntimeError(f'unhandled source range {source_range}')

        return RangeCoords(start, end)

    def spans_and_locs(self, loc):
        '''Generates the spans of a location stepping up through the locations parents.

        The span of the location is generated first, then the span of its parent location,
        recursively.
        '''
        result = []
        while True:
            span = self.lookup_span(loc)
            result.append((span, loc))
            parent_loc = span.macro_parent_loc(loc)
            if parent_loc == -1:
                return result
            loc = parent_loc

    def diagnostic_contexts(self, context):
        # Special diagnostics - those without a source location to highlight, e.g. a
        # complaint about a command-line option, or a compilation summary - only have a
        # single location code and no highlight ranges
        if context.caret_range.start <= location_none:
            assert not context.source_ranges
            context.caret_range = self.range_coords(context.caret_range)
            return [context]

        def intersections(spans, source_range):
            result = []
            if isinstance(source_range, BufferRange):
                start_spans_and_locs = self.spans_and_locs(source_range.start)
                for span in spans:
                    for our_span, our_loc in start_spans_and_locs:
                        if span is not our_span:
                            continue
                        if our_loc == source_range.start:
                            item = source_range
                        else:
                            item = TokenRange(our_loc, our_loc)
                        break
                    else:
                        item = None
                    result.append(item)
                return result

            if isinstance(source_range, SpellingRange):
                # A SpellingRange is a range of characters in a single token's spelling.
                # Remain a SpellingRange for the span with the token's spelling, otherwise
                # decay into a TokenRange for a single token.
                token_loc = source_range.token_loc
                spelling_span, _ = self.spelling_span_and_offset(token_loc)
                our_spans_and_locs = self.spans_and_locs(token_loc)
                for span in spans:
                    for our_span, our_loc in our_spans_and_locs:
                        if span is not our_span:
                            continue
                        if span is spelling_span:
                            item = SpellingRange(our_loc, source_range.start, source_range.end)
                        else:
                            item = TokenRange(our_loc, our_loc)
                        break
                    else:
                        item = None
                    result.append(item)
                return result

            def token_intersection(span, token_spans_and_locs):
                for token_span, token_loc in token_spans_and_locs:
                    if span is token_span:
                        return token_loc
                return None

            if isinstance(source_range, TokenRange):
                start_spans_and_locs = self.spans_and_locs(source_range.start)
                end_spans_and_locs = self.spans_and_locs(source_range.end)
                for span in spans:
                    start = token_intersection(span, start_spans_and_locs)
                    end = token_intersection(span, end_spans_and_locs)
                    if start is None:
                        if end is not None:
                            item = TokenRange(span.start, end)
                        else:
                            item = None
                    else:
                        if end is None:
                            item = TokenRange(start, span.end)
                        else:
                            item = TokenRange(start, end)
                    result.append(item)
                return result

            assert False

        # We are dealing with a diagnostic about source code.
        contexts = []
        # Caret range is an instance of BufferRange, SpellingRange or single token
        # location.  Highlights are token ranges.
        #
        # However, the generic case it is easy to handle, so in case future diagnostic
        # evolution would benefit, the code accepts and handles appropriately any of
        # Buffer Range, SpellingRange, TokenRange or single token locations for the caret
        # range and the highlights.
        caret_spans_and_locs = self.spans_and_locs(context.caret_range.caret_loc())
        caret_spans = [span for span, loc in caret_spans_and_locs]
        caret_ranges = intersections(caret_spans, context.caret_range)
        source_ranges_list = [intersections(caret_spans, source_range)
                              for source_range in context.source_ranges]

        for n, ((span, caret_loc), caret_range) in enumerate(zip(caret_spans_and_locs,
                                                                 caret_ranges)):
            if isinstance(span, BufferSpan):
                did, substitutions = context.did, context.substitutions
            else:
                did, substitutions = span.did_and_substitutions(self.pp, caret_loc)

            source_ranges = [source_ranges_item[n] for source_ranges_item in source_ranges_list]

            # Now convert each range to RangeCoords
            caret_range = self.range_coords(caret_range)
            assert caret_range.start is not None

            source_ranges = [self.range_coords(source_range) for source_range in source_ranges]

            # We finally have the new context; add it to the list
            contexts.append(DiagnosticContext(did, substitutions, caret_loc, caret_range,
                                              source_ranges))

        contexts.reverse()
        return contexts

# Copyright (c) 2025, Neil Booth.
#
# All rights reserved.
#
'''The locator handles the details of token locations - whether they come from source files,
and if so which one, the include stack at the time of that location, the macro stack at the
time of expansion, etc.'''

from bisect import bisect_left
from dataclasses import dataclass
from enum import IntEnum, auto

from ..basic import Buffer, BufferCoords
from ..diagnostics import (
    BufferRange, TokenRange, SpellingRange, DiagnosticContext, DID, location_none,
    RangeCoords,
)


__all__ = ['Locator']


@dataclass(slots=True)
class LineRange:
    '''Names a range of lines in a source file.  A new BufferRange entry creates an initial
    instance.  Subsequent entries are created by #line directives.'''
    start: int           # The first location in this range
    name: str            # Normally a file name, but can be e.g. <scratch>
    line_number: int


class BufferSpan:
    '''Represents the span of locations used for a source file being processed.  If a source
    file is processed more than once (for example, if it is included twice), each usage
    gets its own span.

    An include stack is formed through parent_loc.  This points to the location of the include
    directive (the actual "include" token) so that an include stack can be produced.
    '''

    def __init__(self, buffer, start, parent_loc, name):
        self.buffer = buffer
        self.start = start
        # End is inclusive, so this permits an end-of-buffer location
        self.end = start + len(buffer.text)
        self._parent_loc = parent_loc
        self.line_ranges = [LineRange(start, name, 1)]

    def macro_parent_loc(self, loc):
        assert self.start <= loc <= self.end
        return -1

    def buffer_loc(self, loc):
        assert self.start <= loc <= self.end
        return loc

    def add_line_range(self, start, name, line_number):
        assert start < self.line_ranges[-1].start < start <= self.end
        self.line_ranges.append(LineRange(start, name, line_number))

    def filename(self):
        return self.line_ranges[0].name


class ScratchBufferSpan(Buffer):
    '''A scratch buffer holds the spelling of virtual tokens that are generated by the macro
    expansion process - namely concatenated tokens and stringized tokens.

    The locator creates scratch buffers on demand; there may be several or none in
    existence.  A scratch buffer cannot change size once craeted, as its span of locations
    is fixed and not unbounded.  The scratch buffer keeps track of the origin of the
    virtual tokens it creates so that the macro stack can be correctly reported.
    '''

    def __init__(self, start, end):
        '''Create a scratch buffer with the given size.'''
        super().__init__(bytearray())
        assert start <= end
        self.buffer = self
        self.start = start
        self.end = end
        # Naturally sorted by offset.
        self.entries = []

    def size(self):
        return self.end - self.start + 1

    def add_spelling(self, spelling, parent_loc, entry_kind):
        start = len(self.buffer.text)
        if start + len(spelling) + 1 < self.size():
            # Add the spelling and a newline character (so it appears on its own line in
            # diagnostics)
            self.buffer.text.extend(spelling)
            self.buffer.text.append(10)
            self.entries.append(ScratchEntry(start, parent_loc, entry_kind))
            # Clear the cached line offsets
            self.buffer._sparse_line_offsets = None
            return start
        return -1

    def did_and_substitutions(self, pp, loc):
        kind = self.entry_for_loc(loc).kind
        if kind == ScratchEntryKind.concatenate:
            return DID.in_token_concatenation, []
        else:
            return DID.in_argument_stringizing, []

    def entry_for_loc(self, loc):
        '''Return the parent location (i.e. the location of the ## or # token) of a scratch buffer
        location.  loc is an offset into the scratch buffer.
        '''
        loc -= self.start
        assert 0 <= loc < len(self.buffer.text)
        return self.entries[bisect_left(self.entries, loc + 1, key=lambda e: e.offset) - 1]

    def buffer_loc(self, loc):
        assert self.start <= loc <= self.end
        return loc

    def macro_parent_loc(self, loc):
        return self.entry_for_loc(loc).parent_loc

    def filename(self):
        return '<scratch>'


class ScratchEntryKind(IntEnum):
    concatenate = auto()
    stringize = auto()


@dataclass(slots=True)
class ScratchEntry:
    offset: int
    parent_loc: int
    kind: ScratchEntryKind


class ObjectLikeMacroReplacementSpan:

    def __init__(self, macro, invocation_loc, start):
        self.start = start
        self.end = start + len(macro.replacement_list) - 1
        self.macro = macro
        self.invocation_loc = invocation_loc

    def buffer_loc(self, loc):
        token_index = loc - self.start
        return self.macro.replacement_list[token_index].loc

    def macro_parent_loc(self, loc):
        return self.invocation_loc

    def macro_name(self, pp):
        '''Return the macro name (as UTF-8).'''
        return pp.token_spelling_at_loc(self.invocation_loc)

    def did_and_substitutions(self, pp, loc):
        return DID.in_expansion_of_macro, [self.macro_name(pp)]


class FunctionLikeMacroReplacementSpan(ObjectLikeMacroReplacementSpan):
    pass


@dataclass(slots=True)
class MacroContext:
    macro_loc: int
    span: object


class Locator:
    '''Manages and supplies token locations.'''

    FIRST_BUFFER_LOC = 1
    FIRST_MACRO_LOC = 1 << 40

    def __init__(self, pp):
        self.pp = pp
        self.buffer_spans = []
        self.macro_spans = []
        self.scratch_range = None

    def new_buffer_loc(self, buffer, name, parent_loc):
        buffer_spans = self.buffer_spans
        if buffer_spans:
            start = buffer_spans[-1].end + 1
        else:
            start = self.FIRST_BUFFER_LOC
        buffer_spans.append(BufferSpan(buffer, start, parent_loc, name))
        return start

    def next_macro_span_start(self):
        try:
            return self.macro_spans[-1].end + 1
        except IndexError:
            return self.FIRST_MACRO_LOC

    def functionlike_macro_replacement_span(self, macro, parent_loc):
        start = self.next_macro_span_start()
        self.macro_spans.append(FunctionLikeMacroReplacementSpan(macro, parent_loc, start))
        return start

    def objlike_macro_replacement_span(self, macro, parent_loc):
        start = self.next_macro_span_start()
        self.macro_spans.append(ObjectLikeMacroReplacementSpan(macro, parent_loc, start))
        return start

    def new_scratch_token(self, spelling, parent_loc, entry_kind):
        def alloc_in_current(spelling):
            if self.scratch_range:
                result = self.scratch_range.add_spelling(spelling, parent_loc, entry_kind)
                if result != -1:
                    return self.scratch_range.start + result
            return -1

        assert isinstance(entry_kind, ScratchEntryKind)
        loc = alloc_in_current(spelling)
        if loc == -1:
            size = max(len(spelling), 1_000)
            start = self.buffer_spans[-1].end
            self.scratch_range = ScratchBufferSpan(start, start + size - 1)
            self.buffer_spans.append(self.scratch_range)
            loc = alloc_in_current(spelling)
            assert loc != -1
        return loc

    def lookup_span(self, loc):
        if loc >= self.FIRST_MACRO_LOC:
            spans = self.macro_spans
        else:
            spans = self.buffer_spans
        n = bisect_left(spans, loc + 1, key=lambda lr: lr.start) - 1
        span = spans[n]
        assert span.start <= loc <= span.end
        return span

    def spelling_span_and_offset(self, loc):
        '''Return a pair (span, offset) where span is a BufferSpan or ScratchBufferSpan
        instance.'''
        span = self.lookup_span(loc)
        if isinstance(span, (FunctionLikeMacroReplacementSpan, ObjectLikeMacroReplacementSpan)):
            loc = span.buffer_loc(loc)
            span = self.lookup_span(loc)
        return span, loc - span.start

    def spelling_buffer_and_offset(self, loc):
        '''Return a buffer and offset into it so that the token can be lexed.'''
        span, offset = self.spelling_span_and_offset(loc)
        return span.buffer, offset

    def source_buffer_loc(self, loc):
        while True:
            span = self.lookup_span(loc)
            parent_loc = span.macro_parent_loc(loc)
            if parent_loc == -1:
                return loc
            loc = parent_loc

    def source_file_coords(self, loc):
        return self.buffer_coords(self.source_buffer_loc(loc))

    def diagnostic_contexts_core(self, orig_context):
        def source_buffer_range(source_range):
            start = self.source_buffer_loc(source_range.start)
            end = self.source_buffer_loc(source_range.end)
            return TokenRange(start, end)

        def macro_context_stack(loc):
            contexts = []
            while True:
                span = self.lookup_span(loc)
                parent_loc = span.macro_parent_loc(loc)
                if parent_loc == -1:
                    return contexts
                contexts.append(MacroContext(loc, span))
                loc = parent_loc

        def range_contexts(token_range):
            start_contexts = macro_context_stack(token_range.start)
            if token_range.start == token_range.end:
                end_contexts = start_contexts
            else:
                end_contexts = macro_context_stack(token_range.end)
            return [start_contexts, end_contexts]

        def intersections(span, highlight_contexts):
            ranges = []
            for start_contexts, end_contexts in highlight_contexts:
                start = None
                end = None
                for context in start_contexts:
                    if context.span is span:
                        start = context.macro_loc
                        break
                for context in end_contexts:
                    if context.span is span:
                        end = context.macro_loc
                        break
                if start is None:
                    if end is not None:
                        ranges.append((span.start, end))
                elif end is None:
                    ranges.append((start, span.end))
                else:
                    ranges.append((start, end))

            buffer_loc = span.buffer_loc
            return [TokenRange(buffer_loc(start), buffer_loc(end)) for start, end in ranges]

        def caret_and_spans(orig_context):
            def caret_range_token_loc(source_range):
                if isinstance(source_range, BufferRange):
                    # A BufferRange can be into a standard or scratch buffer.
                    return source_range.start
                if isinstance(source_range, TokenRange):
                    assert source_range.start == source_range.end
                    return source_range.start
                return source_range.token_loc   # SpellingRange

            caret_token_loc = caret_range_token_loc(orig_context.caret_range)
            result = macro_context_stack(caret_token_loc)
            if result:
                for n, context in enumerate(result):
                    caret_loc = context.span.buffer_loc(context.macro_loc)
                    if n == 0 and isinstance(orig_context.caret_range, SpellingRange):
                        caret_range = SpellingRange(caret_loc, orig_context.caret_range.start,
                                                    orig_context.caret_range.end)
                    else:
                        caret_range = TokenRange(caret_loc, caret_loc)
                    result[n] = (caret_loc, caret_range, context.span)

                # Lower the orig_context caret range to a source file
                token_loc = self.source_buffer_loc(caret_token_loc)
                orig_context.caret_range = TokenRange(token_loc, token_loc)
            return result

        contexts = []
        caret_ranges = caret_and_spans(orig_context)

        if caret_ranges:
            highlight_contexts = [range_contexts(source_range)
                                  for source_range in orig_context.source_ranges]
            for caret_loc, caret_range, span in caret_ranges:
                # Now add an extry for each source range that intersects this context level
                source_ranges = intersections(span, highlight_contexts)
                if span is None:
                    # Use the original context but replace its source ranges
                    orig_context.source_ranges = source_ranges
                    context = orig_context
                else:
                    did, substitutions = span.did_and_substitutions(self.pp, caret_loc)
                    context = DiagnosticContext(did, substitutions, caret_loc,
                                                caret_range, source_ranges)
                contexts.append(context)

        # Lower the source ranges in the original context and make it the final context
        orig_context.source_ranges = [source_buffer_range(source_range)
                                      for source_range in orig_context.source_ranges]
        contexts.append(orig_context)
        contexts.reverse()
        return contexts

    def buffer_coords(self, loc):
        '''Convert a location to a BufferCoords instance.'''
        span, offset = self.spelling_span_and_offset(loc)
        buffer = span.buffer
        line_offset, line_number = buffer.offset_to_line_info(offset)
        filename = span.filename()
        return BufferCoords(buffer, filename, line_number, offset - line_offset, line_offset)

    def token_length(self, loc):
        '''The length of the token in bytes in the source file.  This incldues, e.g., escaped
        newlines.  The result can be 0, for end-of-source indicator EOF.
        '''
        lexer = self.pp.lexer_at_loc(loc)
        prior_cursor = lexer.cursor
        lexer.get_token_quietly()
        return lexer.cursor - prior_cursor

    def range_coords(self, source_range):
        if isinstance(source_range, SpellingRange):
            # Convert the SpellingRange to a BufferRange
            assert source_range.start < source_range.end
            lexer = self.pp.lexer_at_loc(source_range.token_loc)
            cursor = lexer.cursor
            lexer.get_token_quietly()
            offsets = [source_range.start, source_range.end]
            # FIXME: this is ugly, find something better
            lexer.utf8_spelling(cursor, lexer.cursor, offsets)
            source_range = BufferRange(offsets[0], offsets[1])

        if isinstance(source_range, BufferRange):
            start = self.buffer_coords(source_range.start)
            end = self.buffer_coords(source_range.end)
            assert start.buffer is end.buffer
        elif isinstance(source_range, TokenRange):
            if source_range.start <= location_none:
                start = end = None
            else:
                start = self.buffer_coords(source_range.start)
                if source_range.start == source_range.end:
                    end = start
                else:
                    end = self.buffer_coords(source_range.end)
                token_end = source_range.end + self.token_length(source_range.end)
                end = self.buffer_coords(token_end)
        else:
            raise RuntimeError(f'unhandled source range {source_range}')

        return RangeCoords(start, end)

    def diagnostic_contexts(self, context):
        '''Expand the diagnostic context stack for the given diagnostic context.'''
        # Apart from the caret range, all ranges must be TokenRange instances
        assert all(isinstance(source_range, TokenRange)
                   for source_range in context.source_ranges)

        # Special ranges don't have source text, and only a single location code
        if context.caret_range.start <= location_none:
            assert not context.source_ranges
            contexts = [context]
        else:
            contexts = self.diagnostic_contexts_core(context)

        for context in contexts:
            context.caret_range = self.range_coords(context.caret_range)
            context.source_ranges = [self.range_coords(source_range)
                                     for source_range in context.source_ranges]

        return contexts
